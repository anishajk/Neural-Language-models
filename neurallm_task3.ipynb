{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarthak55k/Documents/NLP_course/HW5/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sarthak55k/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neurallm_utils as nutils\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "tokenize_by_words = nutils.read_file_spooky(\"./spooky_author_train.csv\",ngram=NGRAM,by_character=False)\n",
    "tokenize_by_char = nutils.read_file_spooky(\"./spooky_author_train.csv\",ngram=NGRAM,by_character=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25374 60\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(tokenize_by_words)\n",
    "encoded_for_words = word_tokenizer.texts_to_sequences(tokenize_by_words)\n",
    "word_vocab = len(word_tokenizer.word_index)\n",
    "\n",
    "char_tokenizer = Tokenizer(char_level=True)\n",
    "char_tokenizer.fit_on_texts(tokenize_by_char)\n",
    "encoded_for_char = char_tokenizer.texts_to_sequences(tokenize_by_char)\n",
    "char_vocab = len(char_tokenizer.word_index)\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "print(word_vocab,char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index Size for Word Tokenizer: 25374\n",
      "Character Index Size for Character Tokenizer: 60\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "word_index_size = len(word_tokenizer.word_index)\n",
    "print(\"Word Index Size for Word Tokenizer:\", word_index_size)\n",
    "\n",
    "# Print the size of the word index for the character tokenizer\n",
    "char_index_size = len(char_tokenizer.word_index)\n",
    "print(\"Character Index Size for Character Tokenizer:\", char_index_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spooky data by character 2957553\n",
      "Spooky data by word 634080\n",
      "\n",
      "Word sequences\n",
      "[1, 1, 32]\n",
      "[1, 32, 2956]\n",
      "[32, 2956, 3]\n",
      "[2956, 3, 155]\n",
      "[3, 155, 3]\n",
      "\n",
      "Character Sequences\n",
      "[21, 21, 3]\n",
      "[21, 3, 9]\n",
      "[3, 9, 7]\n",
      "[9, 7, 8]\n",
      "[7, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    data = []\n",
    "    for l in encoded:\n",
    "        for i in range(len(l)-ngram+1):\n",
    "            temp = l[i:i+ngram]\n",
    "            # temp.append(l[i+ngram-1])\n",
    "            data.append(temp)\n",
    "    \n",
    "    return data\n",
    "\n",
    "        \n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "word_data = generate_ngram_training_samples(encoded_for_words,NGRAM)\n",
    "char_data = generate_ngram_training_samples(encoded_for_char,NGRAM)\n",
    "\n",
    "print(f'Spooky data by character {len(char_data)}')\n",
    "print(f'Spooky data by word {len(word_data)}')\n",
    "\n",
    "print('\\nWord sequences')\n",
    "for i in range(5):\n",
    "    print(word_data[i])\n",
    "\n",
    "print('\\nCharacter Sequences')\n",
    "for i in range(5):\n",
    "    print(char_data[i])\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X for word: 634080\n",
      "Length of y for word: 634080\n",
      "Length of X for char: 2957553\n",
      "Length of y for char: 2957553\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "\n",
    "\n",
    "X_char = []\n",
    "y_char = []\n",
    "\n",
    "def XY_split(data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for l in data:\n",
    "        X.append(l[:-1])\n",
    "        y.append(l[-1])\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "X_word, y_word = XY_split(word_data)\n",
    "X_char, y_char = XY_split(char_data)\n",
    "# print out the shapes to verify that they are correct\n",
    "print(f'Length of X for word: {len(X_word)}')\n",
    "print(f'Length of y for word: {len(y_word)}')\n",
    "print(f'Length of X for char: {len(X_char)}')\n",
    "print(f'Length of y for char: {len(y_char)}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    word_to_embeddings = {}\n",
    "    index_to_embeddings = {}\n",
    "\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        word_to_embeddings[word] = w2v_model[word]\n",
    "        index_to_embeddings[index] = w2v_model[word]\n",
    "    \n",
    "    return word_to_embeddings, index_to_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "word_embeddings, word_index_to_embeddings = read_embeddings(\"./spooky_embedding_word.txt\",word_tokenizer)\n",
    "char_embeddings, char_index_to_embeddings = read_embeddings(\"./spooky_embedding_char.txt\",char_tokenizer)\n",
    "\n",
    "\n",
    "padding_token = \"[PAD]\"\n",
    "padding_index = 0\n",
    "padding_vector = np.zeros(EMBEDDINGS_SIZE)\n",
    "\n",
    "# For word\n",
    "word_tokenizer.word_index[padding_token] = padding_index\n",
    "word_tokenizer.index_word[padding_index] = padding_token\n",
    "\n",
    "word_index_to_embeddings[padding_index] = padding_vector\n",
    "word_embeddings[padding_token] = padding_vector\n",
    "\n",
    "# For char\n",
    "char_tokenizer.word_index[padding_token] = padding_index\n",
    "char_tokenizer.index_word[padding_index] = padding_token\n",
    "\n",
    "char_index_to_embeddings[padding_index] = padding_vector\n",
    "char_embeddings[padding_token] = padding_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict, for_feedforward: bool, num_classes: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    num_samples = len(X)\n",
    "    indices = list(range(num_samples))\n",
    "    while True:\n",
    "        # Shuffle the data at the start of each epoch (optional)\n",
    "        np.random.shuffle(indices)  # Shuffle the data for each epoch\n",
    "        for start in range(0, num_samples, num_sequences_per_batch):\n",
    "            end = min(start + num_sequences_per_batch, num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch_X = [X[i] for i in batch_indices]\n",
    "            batch_y = [y[i] for i in batch_indices]\n",
    "\n",
    "            # Convert text sequences to embeddings\n",
    "            batch_embeddings = []\n",
    "            for sequence in batch_X:\n",
    "                if for_feedforward:\n",
    "                    # For feedforward, concatenate embeddings of each word\n",
    "                    sequence_embeddings = [index_2_embedding[word] for word in sequence]\n",
    "                    sequence_embedding = np.concatenate(sequence_embeddings)\n",
    "                else:\n",
    "                    # For RNN, keep the sequence of embeddings\n",
    "                    sequence_embedding = [index_2_embedding[word] for word in sequence]\n",
    "                batch_embeddings.append(sequence_embedding)\n",
    "\n",
    "            # Convert labels to one-hot vectors\n",
    "            batch_labels = to_categorical(batch_y,num_classes)\n",
    "\n",
    "            yield np.array(batch_embeddings), batch_labels\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X for words in a batch: (128, 100)\n",
      "Shape of y for words in a batch: (128, 25375)\n",
      "Shape of X for chars in a batch: (128, 100)\n",
      "Shape of y for chars in a batch: (128, 61)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "word_generator = data_generator(X_word,y_word,128,word_index_to_embeddings,True,word_vocab+1)\n",
    "char_generator = data_generator(X_char,y_char,128,char_index_to_embeddings,True,char_vocab+1)\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "sample_word = next(word_generator)\n",
    "sample_char = next(char_generator)\n",
    "# Examples:\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "print(f'Shape of X for words in a batch: {sample_word[0].shape}')\n",
    "print(f'Shape of y for words in a batch: {sample_word[1].shape}')\n",
    "\n",
    "\n",
    "print(f'Shape of X for chars in a batch: {sample_char[0].shape}')\n",
    "print(f'Shape of y for chars in a batch: {sample_char[1].shape}')\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               12928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 25375)             837375    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 860639 (3.28 MB)\n",
      "Trainable params: 860639 (3.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               12928     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 61)                2013      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25277 (98.74 KB)\n",
      "Trainable params: 25277 (98.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "def generate_model(input_dim,output_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model\n",
    "    model.add(Dense(units=128, activation='relu', input_shape=(input_dim,)))  # Input layer\n",
    "    model.add(Dense(units=64, activation='relu'))  # Hidden layer\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    # model.add(Dense(units=32, activation='relu'))\n",
    "    # model.add(Dense(units=16, activation='relu'))\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))  # Output layer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    new_model = keras.models.clone_model(model)\n",
    "    new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "\n",
    "word_model_FNN = generate_model((NGRAM-1)*EMBEDDINGS_SIZE,word_vocab+1)\n",
    "char_model_FNN = generate_model((NGRAM-1)*EMBEDDINGS_SIZE,char_vocab+1)\n",
    "# Compile the model\n",
    "# char_model_FNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Print a summary of the model's architecture\n",
    "word_model_FNN.summary()\n",
    "char_model_FNN.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4953/4953 [==============================] - 56s 11ms/step - loss: 5.7296 - accuracy: 0.1942\n",
      "Epoch 2/5\n",
      "4953/4953 [==============================] - 56s 11ms/step - loss: 5.2581 - accuracy: 0.2062\n",
      "Epoch 3/5\n",
      "4953/4953 [==============================] - 58s 12ms/step - loss: 5.1053 - accuracy: 0.2092\n",
      "Epoch 4/5\n",
      "4953/4953 [==============================] - 63s 13ms/step - loss: 5.0168 - accuracy: 0.2108\n",
      "Epoch 5/5\n",
      "4953/4953 [==============================] - 63s 13ms/step - loss: 4.9598 - accuracy: 0.2118\n",
      "Epoch 1/5\n",
      "4953/4953 [==============================] - 4s 864us/step - loss: 2.1152 - accuracy: 0.3692\n",
      "Epoch 2/5\n",
      "4953/4953 [==============================] - 4s 822us/step - loss: 2.0086 - accuracy: 0.3854\n",
      "Epoch 3/5\n",
      "4953/4953 [==============================] - 4s 820us/step - loss: 1.9942 - accuracy: 0.3860\n",
      "Epoch 4/5\n",
      "4953/4953 [==============================] - 4s 812us/step - loss: 1.9866 - accuracy: 0.3865\n",
      "Epoch 5/5\n",
      "4953/4953 [==============================] - 4s 862us/step - loss: 1.9780 - accuracy: 0.3878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ef08aee0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n",
    "word_model_FNN.fit(x=word_generator,steps_per_epoch=len(X_word)/128,epochs=5)\n",
    "char_model_FNN.fit(x=char_generator,steps_per_epoch=len(X_word)/128,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_model_FNN/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_model_FNN/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_model_FNN/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_model_FNN/assets\n"
     ]
    }
   ],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "word_model_FNN.save('word_model_FNN')\n",
    "char_model_FNN.save('char_model_FNN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "word_model_FNN = keras.saving.load_model(\"word_model_FNN\")\n",
    "char_model_FNN = keras.saving.load_model(\"char_model_FNN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list,\n",
    "                 index_to_embeddings: dict,\n",
    "                 char: bool):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    result = seed.copy()\n",
    "    i = 0\n",
    "    while result[-1]!='</s>':\n",
    "        input_sequence = tokenizer.texts_to_sequences([result])[0]\n",
    "        input_sequence = input_sequence[-2:]\n",
    "        sequence_embedding = [index_to_embeddings[word] for word in input_sequence]\n",
    "        sequence_embedding = np.array(np.concatenate(sequence_embedding))\n",
    "        sequence_embedding = sequence_embedding.reshape(1,-1)\n",
    "        # padded_sequence = pad_sequences([input_sequence], maxlen=max_length-1, padding='pre')\n",
    "        predictions = model.predict(sequence_embedding,verbose=False)\n",
    "        # predicted_word_index = np.argmax()\n",
    "        top5_indices = np.argsort(predictions)[0][-5:]\n",
    "        # if predicted_word_index == 0:  # Check for the end of sentence token\n",
    "        predicted_word_index = np.random.choice(top5_indices)\n",
    "        \n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        if predicted_word == '<s>':\n",
    "            pass\n",
    "        if predicted_word == '</s>':\n",
    "            break\n",
    "        result.append(predicted_word)\n",
    "    \n",
    "    if char:\n",
    "        return re.sub('_',' ', \"\".join(result))\n",
    "    else:\n",
    "        generated_sentence = ' '.join(result)\n",
    "        return generated_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` you know how he was the first thing that , i could not fail . '' '' of a man had no more to a little woman had been a great time the old woman was , the whole day and a little , and then of a little woman 's head and with his head to her and to my friend .\n",
      " mouggint th anctoned,',,,\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "seed = ['<s>']*(NGRAM-1)\n",
    "print(generate_seq(word_model_FNN,word_tokenizer,seed,word_index_to_embeddings,False)[8:])\n",
    "print(generate_seq(char_model_FNN,char_tokenizer,seed,char_index_to_embeddings,True)[7:])\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "def generate_examples(n,model,tokenizer,seed,embeddings,type_char,filename):\n",
    "    f = open(filename,'w')\n",
    "    for i in range(n):\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        generated_text = generate_seq(model,tokenizer,seed,embeddings,type_char)+\"\\n\"\n",
    "        generated_text = re.sub(\"<s>\",\"\",generated_text)\n",
    "        f.write(generated_text)\n",
    "\n",
    "generate_examples(100,char_model_FNN,char_tokenizer,seed,char_index_to_embeddings,True,'char_sents.txt')\n",
    "generate_examples(100,word_model_FNN,word_tokenizer,seed,word_index_to_embeddings,False,'word_sents.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
